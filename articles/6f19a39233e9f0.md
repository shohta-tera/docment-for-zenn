---
title: "Airflowã§ç•°ãªã‚‹DAGã¨ã®é–“ã«ä¾å­˜é–¢ä¿‚ã‚’è¨­å®šã™ã‚‹"
emoji: "ğŸˆ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Airflow", "DAG", "python", "workflow"]
published: true
---


# ã¯ã˜ã‚ã«

# ç’°å¢ƒ
- WSL2: Ubuntu 20
- Helm: v3.6.1
- Airflow: ver2

# å‰å›ã®è¨˜äº‹

[å‰å›ã®è¨˜äº‹](https://zenn.dev/articles/8027cf8283e559)ã§ã¯ã€Airflowã§ã®ã‚¸ãƒ§ãƒ–ã®ä½œæˆæ–¹æ³•ã®ç´¹ä»‹ã‚’è¡Œã„ã¾ã—ãŸã€‚ä»Šå›ã¯ã€DAGé–“ã§ã®ä¾å­˜é–¢ä¿‚ã®è¨­å®šæ–¹æ³•ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¦ã„ãã¾ã™ã€‚

## è¨˜äº‹ã®å†…å®¹

å‰å›ã®ç´¹ä»‹ã®è¨˜äº‹ã§ã¯ã€ã‚ã‚‹å‡¦ç†ãŒæˆåŠŸã—ãŸã®ã‚’ç¢ºèªã—ãŸå¾Œã«ã€æ¬¡ã®å‡¦ç†ã‚’å®Ÿæ–½ã™ã‚‹ã‚ˆã†ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ãŠã„ã¦ä¾å­˜é–¢ä¿‚ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã‚¿ã‚¹ã‚¯ã®åŒæœŸå‡¦ç†ã‚’å®šç¾©ã§ãã¾ã—ãŸã€‚(ä¾‹: DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã€csvåŒ–ã—ã¦AWS S3ã«ä¿å­˜ã™ã‚‹ãªã©ã€‚)  
APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–ã£ã¦ãã‚‹ã‚¸ãƒ§ãƒ–ã‚’å®šç¾©ã—ãŸã„ã¨ããªã©ã«ã€DAGã‚’ç´°ã‹ãä¿ã¡ãŸã„ã®ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã”ã¨ã«DAGã‚’è¤‡æ•°ä½œæˆã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒå‡ºã¦ãã¾ã™ã€‚ã“ã®ã‚ˆã†ã«DAGãŒåˆ†å‰²ã•ã‚ŒãŸå ´åˆã§ã¯ã€åˆ¥ã®DAGã§å®šç¾©ã—ãŸã‚¿ã‚¹ã‚¯ãŒæˆåŠŸã—ãŸã®ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã€DAGã‚’å®Ÿè¡Œã—ãŸã„ã‚±ãƒ¼ã‚¹ã«ã¯ä¸Šè¨˜æ‰‹æ³•ã¯é©å¿œã§ãã¾ã›ã‚“ã€‚ãã“ã§ã€ä»Šå›ç´¹ä»‹ã™ã‚‹ã®ã¯`ExternalTaskSensor`ã‚’ç”¨ã„ã¦ã€DAGã®ä¾å­˜é–¢ä¿‚ã‚’åˆ¥DAGã«ã¾ã§åºƒã’ã‚‹æ‰‹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚  
ä»Šå›ã®è¨˜äº‹ã§ç´¹ä»‹ã—ãŸã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®ãƒ¬ãƒã‚¸ãƒˆãƒªã«ã‚ã‚Šã¾ã™ã€‚
https://github.com/shohta-tera/workflows/tree/main/src/dags/task_dependency

## TL;DR

DAGé–“ã§ã®ã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ã‚’è¨­å®šã™ã‚‹ã«ã¯ã€`ExternalTaskSensor`ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

# DAGã®è¨­å®š

ä»Šå›è¨­å®šã™ã‚‹DAGã¨ã—ã¦ã€DAG_Aã®ã‚¿ã‚¹ã‚¯(alpha)ãŒæˆåŠŸã™ã‚‹ã®ã‚’ç¢ºèªã—ã¦ã‹ã‚‰DAG_Bã®ã‚¿ã‚¹ã‚¯(beta)ã‚’å®Ÿè¡Œã™ã‚‹ã€‚ã¨ã—ã¾ã™ã€‚

## DAG_A
å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã€ãŸã printã—ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚

``` python
from logging import INFO, basicConfig, getLogger

from airflow.decorators import dag, task
from airflow.utils.dates import days_ago
from kubernetes.client import models as k8s

basicConfig(level=INFO)
logger = getLogger(__name__)
default_args = {"owner": "admin", "retries": 1}


# dag configuration
@dag(
    default_args=default_args,
    schedule_interval=None,
    start_date=days_ago(1),
    tags=["sample-dag"],
)
# DAG name
def DAG_A():
    @task(
        executor_config={
            "pod_override": k8s.V1Pod(
                spec=k8s.V1PodSpec(
                    containers=[
                        k8s.V1Container(
                            name="airflow-worker",
                            resources=k8s.V1ResourceRequirements(
                                limits={"cpu": "1000m", "memory": "1Gi"},
                                requests={"cpu": "250m", "memory": "256Mi"},
                            ),
                        )
                    ]
                )
            )
        }
    )
    def alpha():
        print("test")

    alpha()


dag = DAG_A()

```

## DAG_B

### TaskFlowSensorã®è¨­å®š

```python
    @provide_session
    def _get_execution_date_of_task_a(exec_date, session=None, **kwargs):
        dag_last_run = get_last_dagrun("DAG_A", session)
        return dag_last_run.execution_date
    
    task_a_sensor = ExternalTaskSensor(
        task_id = "alpha_sensor",
        external_dag_id="DAG_A",
        external_task_id="alpha",
        allowed_states=["success"],
        execution_date_fn=_get_execution_date_of_task_a
    )
```

ExternalTaskSensorã®è¨­å®šã™ã‚‹é …ç›®ã«ã¤ã„ã¦ã€èª¬æ˜ã—ã¦ã„ãã¾ã™ã€‚
- task_id: ExternalTaskSensorã®å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯å
- external_dag_id: ä¾å­˜é–¢ä¿‚ã‚’è¦‹ã‚‹DAGå
- external_task_id: ä¸Šè¨˜ã®DAGã®ä¸­ã§çŠ¶æ…‹ã‚’ç›£è¦–ã—ãŸã„ã‚¿ã‚¹ã‚¯ã‚’æŒ‡å®šã™ã‚‹
- allowed_states: æŒ‡å®šã—ãŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ã¨ãã®ã¿å¾Œç¶šã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹
- execution_date_fn: ä»Šå›ã¯æœ€æ–°ã«å®Ÿè¡Œã•ã‚ŒãŸDAGã®çŠ¶æ…‹ã‚’ç›£è¦–ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’æŒ‡å®šã—ã¾ã—ãŸã€‚ç‹¬è‡ªã«è¨­å®šã§ãã¾ã™ã€‚é–¢æ•°ã‚’ä½œæˆã›ãšã€æŒ‡å®šã—ãªã„å ´åˆã¯ã€`execution_delta`ã§ã€`datetime.timedelta`ã§æŒ‡å®šã—ãŸå·®åˆ†ã®æ™‚é–“ã«å®Ÿè¡Œã•ã‚ŒãŸDAGã‚’è¦‹ã¾ã™ã€‚

### TaskFlowAPIã«ãŠã‘ã‚‹ä¾å­˜é–¢ä¿‚ã®è¨­å®šæ–¹æ³•

```python
    beta_task = beta()
    task_a_sensor >> beta_task
```
TaskFlowAPIã‚’ä½¿ç”¨ã™ã‚Œã°ã€ç°¡å˜ã«ã‚¿ã‚¹ã‚¯å®šç¾©ã‚’ä½œæˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ãŒã€ä»Šå›ã®ã‚ˆã†ã«ã€ExternalTaskSensorã‚’çµ„ã¿åˆã‚ã›ã‚‹éš›ã«ã¯ã€å®šç¾©ã®ä»•æ–¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚  
ä¸€æ—¦ã‚¿ã‚¹ã‚¯ã‚’å¤‰æ•°ã¨ã—ã¦æŒã£ã¦ã€å¾“æ¥ã®Airflowã§ã‚ã£ãŸã‚ˆã†ãª`>>`ã‚’ç”¨ã„ã¦ä¾å­˜é–¢ä¿‚ã‚’è¨­å®šã—ã¾ã™ã€‚  

### DAGã®å…¨ä½“åƒ

ä¸Šè¨˜ã®ã‚‚ã®ã‚’çµ„ã¿åˆã‚ã›ã¦ã€DAG_Aã®Task`alpha`ãŒ`success`ã®å ´åˆã®ã¿ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹DAGã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚
```python
from logging import INFO, basicConfig, getLogger

from airflow.decorators import dag, task
from airflow.models.dag import get_last_dagrun
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.dates import days_ago
from airflow.utils.session import provide_session
from kubernetes.client import models as k8s

basicConfig(level=INFO)
logger = getLogger(__name__)
default_args = {"owner": "admin", "retries": 1}


# dag configuration
@dag(
    default_args=default_args,
    schedule_interval=None,
    start_date=days_ago(1),
    tags=["sample-dag"],
)
# DAG name
def DAG_B():
    @provide_session
    def _get_execution_date_of_task_a(exec_date, session=None, **kwargs):
        dag_last_run = get_last_dagrun(
            "DAG_A", session, include_externally_triggered=True
        )
        return dag_last_run.execution_date

    task_a_sensor = ExternalTaskSensor(
        task_id="alpha_sensor",
        external_dag_id="DAG_A",
        external_task_id="alpha",
        allowed_states=["success"],
        execution_date_fn=_get_execution_date_of_task_a,
    )

    @task(
        executor_config={
            "pod_override": k8s.V1Pod(
                spec=k8s.V1PodSpec(
                    containers=[
                        k8s.V1Container(
                            name="airflow-worker",
                            resources=k8s.V1ResourceRequirements(
                                limits={"cpu": "1000m", "memory": "1Gi"},
                                requests={"cpu": "250m", "memory": "256Mi"},
                            ),
                        )
                    ]
                )
            )
        }
    )
    def beta():
        print("test")

    beta_task = beta()
    task_a_sensor >> beta_task


dag = DAG_B()

```

### DAG_Bã®è©³ç´°

![](/images/6f19a39233e9f0/dag_b.png)
ä¸Šè¨˜å›³ã¯ã€DAG_Bã®çµæœã¨ãªã£ã¦ãŠã‚Šã€`alpha_sensor`ã¨ã„ã†åå‰ã®ã‚¿ã‚¹ã‚¯ã§DAG_Aã®`alpha`ã®ã‚¿ã‚¹ã‚¯ã®çµæœã‚’è¦‹ã¦ã„ã¾ã™ã€‚  

#### alpha_sensorã®ãƒ­ã‚°

```
[2022-01-30 14:30:44,994] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: DAG_B.alpha_sensor 2022-01-30T14:30:21.760391+00:00 [queued]>
[2022-01-30 14:30:45,014] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: DAG_B.alpha_sensor 2022-01-30T14:30:21.760391+00:00 [queued]>
[2022-01-30 14:30:45,015] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2022-01-30 14:30:45,015] {taskinstance.py:1069} INFO - Starting attempt 1 of 2
[2022-01-30 14:30:45,015] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2022-01-30 14:30:45,024] {taskinstance.py:1089} INFO - Executing <Task(ExternalTaskSensor): alpha_sensor> on 2022-01-30T14:30:21.760391+00:00
[2022-01-30 14:30:45,029] {standard_task_runner.py:52} INFO - Started process 32 to run task
[2022-01-30 14:30:45,035] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'DAG_B', 'alpha_sensor', '2022-01-30T14:30:21.760391+00:00', '--job-id', '13', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/git_workflow/task_dependency/dag_b.py', '--cfg-path', '/tmp/tmpe_gfo6vn', '--error-file', '/tmp/tmpk3_bt8g7']
[2022-01-30 14:30:45,036] {standard_task_runner.py:77} INFO - Job 13: Subtask alpha_sensor
[2022-01-30 14:30:45,111] {logging_mixin.py:104} INFO - Running <TaskInstance: DAG_B.alpha_sensor 2022-01-30T14:30:21.760391+00:00 [running]> on host dagbalphasensor.97320d86172b4f978611e80f5565750c
[2022-01-30 14:30:45,191] {taskinstance.py:1281} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=admin
AIRFLOW_CTX_DAG_ID=DAG_B
AIRFLOW_CTX_TASK_ID=alpha_sensor
AIRFLOW_CTX_EXECUTION_DATE=2022-01-30T14:30:21.760391+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-01-30T14:30:21.760391+00:00
[2022-01-30 14:30:45,205] {external_task.py:153} INFO - Poking for DAG_A.alpha on 2022-01-30T14:21:50.631744+00:00 ... 
[2022-01-30 14:30:45,216] {base.py:245} INFO - Success criteria met. Exiting.
[2022-01-30 14:30:45,234] {taskinstance.py:1185} INFO - Marking task as SUCCESS. dag_id=DAG_B, task_id=alpha_sensor, execution_date=20220130T143021, start_date=20220130T143044, end_date=20220130T143045
[2022-01-30 14:30:45,287] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-01-30 14:30:45,328] {local_task_job.py:146} INFO - Task exited with return code 0
```

ãƒ­ã‚°ã‹ã‚‰ã‚ã‹ã‚‹ã‚ˆã†ã«ã€DAG_Aã®ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã‚’è¦‹ã¦ã€å¾Œç¶šã®`beta`ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚

## ãŠã‚ã‚Šã«

ä»Šå›ã®è¨˜äº‹ã§ç´¹ä»‹ã™ã‚‹æ–¹æ³•ã‚’ä½¿ç”¨ã™ã‚Œã°ã€DAGå†…ã§ã®ã‚¿ã‚¹ã‚¯é–“ã®ä¾å­˜é–¢ä¿‚ã ã‘ã§ãªãã€DAGé–“ã§ã®ã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã‚’é§†ä½¿ã™ã‚‹ã“ã¨ã§Airflowã«ãŠã„ã¦ã€è‡ªç”±ã«ä¾å­˜é–¢ä¿‚ã‚’ä½œæˆã§ãã€DAGè‡ªä½“ã‚’å°ã•ãã‚·ãƒ³ãƒ—ãƒ«ã«ä¿ã¤ã“ã¨ã‚‚ã§ãã¾ã™ã€‚  
ã¾ãŸã€Airflowã®æä¾›ã™ã‚‹Sensorã«ã¯ã€è¤‡æ•°ç¨®é¡ãŒã‚ã‚Šã€Cloud Pub/Subã‚’å¾…ã¤`PubSubPullSensor`ã ã£ãŸã‚Šã€S3, GCSãªã©ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®å¤‰æ›´ã‚’å¾…ã¤`S3KeySensor`ã‚„`CoogleCloudStorageObjectSensor`ãªã©ãŒã‚ã‚Šã€ETLã‚¸ãƒ§ãƒ–ã§æƒ³å®šã™ã‚‹æ§˜ã€…ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ã¯å¯¾å¿œã§ããã†ã§ã™ã€‚